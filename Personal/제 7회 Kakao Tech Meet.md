---  

# *1. AI Agent 기반 스마트 AI 마이노트*

과거에도 연말평가의 자동화시도가 존재함.
- Jira, Slack같은 tool을 활용함과 동시에 KPI 같은 기준을 결합하여 시도
- But, 모든 상황과 현상에 대한 예외케이스 처리할 수 없었음.

사용자를 위한 AI Agent 서비스?
- 대부분은 Chat Interface
- 앞으로의 방향성이 알잘딱갈센

주요기능 1. 성과 노트 작성
- 데이터 소스의 다양성
- 맥락 파악
- 회사 평가기준 반영

주요기능 2. 성과 평가 초안 작성
- AAT, SBI 피드백
- 개발 과정에서의 어려움
	Data Preprocessing
	Prompt Engineering
		- 요약데이터, 원본데이터, 구조화 변경 (매우 중요하며 수많은 시행착오가 존재함)
- 실제 적용시 고려해야할 리스크
	- AI 윤리 원칙
	- AI 편향성
	- 개인 정보 보호
	- 노동법 고용 규정
	- 다양한 업무 관리 방식
	- 비용 관리

주요기능 3. 


**개인 소감**
- 온전한 Full Automation이 아님. 중간에 human interaction이 끼게됨.
- Closed한 task scope에서 구현한 것으로 보임.
- Agent가 스스로 필요한 질문을 산정하여 물어본다고 하였는데, 정말 생성한 것일까 싶음.
- 최종 결과물로 export되는 AAT(Action Appreciation Thanks), SBI(Situation Behavior Impact) 피드백은 미리 정해놓은 것으로 보여짐.
- AI 편향성에 대한 주기적인 모니터링을 했다고 하는데 모니터링시 편향여부의 자체기준을 선정한 걸까?


# *2. TDD로 앞서가는 프론트엔드 : 디자인, API 없이도 개발을 시작하는 방법*

웹서비스의 병목지점에 있는 FE의 부하를 어떻게 줄일 수 있는지 항상 고민

"아직 FE 개발이 진행 중입니다." -> 흔한 개발 지연 사유

그 이유는타 협업자의 단계가 수행된 이후에야 작업할 수 있음.

"굳이 다른 협업자의 작업을 기다리지 않고 작업할 수는 없나?"
- 기획서를 보고 일정산정
- PoC 및 기술검증
- 기술스택 논의
- 페이즈에 따른 개발 환경 논의
- 배포방식 논의
- 개발 인프라 세팅
- 공통 컴포넌트 및 유틸 개발

문제 해결의 키워드
-> Component & Test(단위)

일반적인 FE 개발 플로우에서 벗어나 화면이 아닌 테스트를 보고 개발 할 수 있다면 생산성이 올라갈 것.


**개인 소감**
- FE 관심없지만 테스트가 화면을 대체한다는 인사이트는 매력적
- FE라는 분야에서 병목을 해결하기 위한 TDD활용의 아이디에이션은 좋은 듯.

# *3. 업무 효율화를 위한 카카오 사내봇 개발기*

Introduction
- 많은서비스에 따라 서로다른 형태의 테이블 구조
- 타 서비스에 대한 데이터 분석이 어려움.

0뎁스 데이터봇
- RAG 구조 채용
- 많은 데이터변경으로 인해 RAG를 사용

Process
- table에 대한 description이 혼재함.
- Document 생성 프로셋스
	- 수집, 가공, 생성, 검수(사람)
- Retriever
	- 테이블 명을 정확히 검색했음에도 낮은 스코어로 검색됨.
	- 칼럼 명을 정확히 검색했음에도 테이블을 찾지 못함.
	- 원인 분석
		- Document에 담겨있는 정보가 많아서 유사도가 희석됨.
		- 즉, 데이터 특징에 따라 분리해서 Pool을 생성
			- 서비스 이름
			- 테이블 이름
			- 테이블 설명
			- 테이블 칼럼

- LLM
	- 적절한 문서만 뽑는것이 관건
	- 목표에 fit한 내용을 반영하는 것이 관건
		- 범용성 필요없음
		- 사내용어 학습 필요
		- 7B 이하 모델 학습
	- 학습 데이터셋 직접 구축
		- LIMA 참조
		- 정답 테이블의 랭크비율을 비슷하게 유지?
	- 질문의도에 맞는 답변을 생성하지 못하는 issue 발생
		- 즉, Task별 전문모델 개발
		- 이때, RAFT 사용
		- Multi-LoRA Adapter를 베이스모델에 붙여 서비스 앞단에 쿼리 유형에 따라 선택하게 함.

- History
	- history유형을 분리하여 2가지로 구분
		- 새로운 맥락 질의
			- 히스토리 삭제
		- 이어지는 질의
			- 앞선 대화를 히스토리로 사용

**Conclusion**
- Garbage in Garbage out
- Query 유형에 따라 개별 Pool을 구축
- LLM은 비용싸움, Task별 전문 모델 선호
- 히스토리 관리가 필수적


**개인 소감**
- 각 서비스별 table에 대한 description이 이미 존재하니 가능했던 것으로 보임.
- 각 Lexical & Semantic Search를 Pool에 대해서 각각 top 3를 뽑는것인가?
- Scoring은 어떤식으로 하는가?
- 200개 평가셋은 어떤식으로 만들었는가?
- 결국 Query Intent에 의존할 수밖에 없는가?


검색 플로우에서 Lexical, Semantic Search시 3개의 Pool에 대해 각각 top 3의 결과를 검색하고 Scoring을 통해 그 중에서 top 3를 뽑아 최종적인 검색결과로 활용하신것인지 궁금하며 이때, Scoring을 어떤방식으로 하셨는지 궁금합니다.


"업무 효율화를 위한 카카오 사내봇 개발기"에 관해 궁금한 점이 있습니다. 성능평가를 위한 평가셋을 약 200개 QA를 사용하셨다고 자료에 기재되어있는데, 이때 평가셋 구축과정이 궁금합니다.
	- 처음에는 LLM으로 시도, 근데 학습 셋이랑 비슷하게 나와서 pending. 내부적으로 demo page를 만들고 그 사람들이 직접 사용한 log를 수집해서 gt를 만들었다.

"업무 효율화를 위한 카카오 사내봇 개발기" 아키텍쳐에 따르면 사용자 쿼리 유형이 특정된 환경에서 유효한 구조로 보여지는데, 전사확대를 진행하신다고 하신 경우 쿼리유형이 많이 늘어날 것이라고 생각이 듭니다. 그때마다 하나하나 정해놓기도 많은 소요가 있을 것 같은데 사용자 쿼리 유형에 구애받지 않은 인사이트가 있으실까요?

사내봇 개발 시 멀티턴에서 새로운 질의인지 이어지는 질문인지 어떻게 판단하나요?
	- Generation 모델이 판단한다..? 아무래도 Routing을 LLM으로 하는듯.

AI Agent 플랫폼 개발 세션과 관련하여, 실제 서비스 운영 시 발생할 수 있는 hallucination(환각) 문제를 어떻게 해결하셨는지 궁금합니다.
	- semantic entropy와 같은 지표를 활용할 수 있지만, 사실 나오는 결과를 보고 환각여부를 정하는 것을 선호한다.

이 제품을 LLM 으로 구현한 Data Catalogue 라고 이해해도 될까요? 만약 그렇다면, 각 팀의 문서 관리 상태에 따라 테이블들의 퀄리티가 현저하게 다를 것 같아서, 전체적인 퀄리티를 어떻게 상향 평준화 할 계획인지 궁금합니다
	- 상향평준화는 어렵다. 데이터 퀄리티를 결정하는게 아닌, 잘 이쁘게 전처리하는 (몇단계의 전처리 step을 활용했다고 함.)


